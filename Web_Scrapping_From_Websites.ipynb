{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Web Scraping Practice: Wikipedia, Quotes to Scrape, and Books to Scrape**\n"
      ],
      "metadata": {
        "id": "0QFVpgklzf7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementing Web Scraping in Python with BeautifulSoup**\n",
        "\n",
        "Web scraping is a powerful technique for extracting data from websites. BeautifulSoup, a Python library, provides tools for parsing HTML and navigating the parse tree, making it an excellent choice for web scraping tasks.\n",
        "\n",
        "### **Extracting Data from Websites**\n",
        "\n",
        "There are two main approaches to extract data from a website:\n",
        "\n",
        "1. **Using the Website's API**: Some websites provide APIs (Application Programming Interfaces) that allow developers to retrieve data in a structured format. For example, Facebook offers the Facebook Graph API for accessing data posted on the platform.\n",
        "\n",
        "2. **Web Scraping**: When a website doesn't offer an API, we can access the HTML of its webpages and extract useful information directly. This technique, known as web scraping, web harvesting, or web data extraction, involves sending HTTP requests to the website and parsing the HTML content.\n",
        "\n",
        "### **Steps Involved in Web Scraping**\n",
        "\n",
        "1. **Sending HTTP Requests**: We start by sending an HTTP request to the URL of the webpage we want to access. Python's `requests` library is commonly used for this task.\n",
        "\n",
        "2. **Parsing HTML Content**: Once we receive the HTML content of the webpage, we need to parse it to extract the desired data. HTML parsing libraries like `Beautiful Soup` help create a nested/tree structure of the HTML data.\n",
        "\n",
        "3. **Navigating and Searching the Parse Tree**: With the parsed HTML content, we can navigate and search the parse tree to locate specific elements and extract relevant information. Beautiful Soup provides convenient methods for traversing the parse tree and extracting data.\n",
        "\n",
        "By following these steps, we can efficiently scrape data from websites and use it for various purposes, such as data analysis, research, and automation.\n",
        "\n"
      ],
      "metadata": {
        "id": "-dmr9kWl1Fpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- The `!` symbol at the beginning of the line tells Colab to execute the command as a shell command rather than a Python statement.\n",
        "- `pip` is a package management system used to install and manage software packages written in Python.\n",
        "- `install` is a pip command that is used to install packages.\n",
        "- `requests` and `beautifulsoup4` are the names of the packages you want to install. These are the packages required for web scraping.\n",
        "\n",
        "By running this command in a cell in your GitHub Colab file, you'll install the necessary packages, and then you can proceed with your web scraping tasks using `requests` and `BeautifulSoup` in Python.\n"
      ],
      "metadata": {
        "id": "Ubw1woPVzgF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7mdZhCkzd8l",
        "outputId": "25c68f9b-e84d-4c38-d94f-c6c6f25d2d43"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three website links from where we can scrape data:\n",
        "\n",
        "1. **Wikipedia**: https://www.wikipedia.org/\n",
        "   - Wikipedia is a free online encyclopedia with articles covering a wide range of topics. It provides valuable information on various subjects, making it a useful source for web scraping.\n",
        "\n",
        "2. **Quotes to Scrape**: http://quotes.toscrape.com/\n",
        "   - Quotes to Scrape is a website specifically designed for practicing web scraping. It contains a collection of quotes from various authors, along with their tags.\n",
        "\n",
        "3. **Books to Scrape**: http://books.toscrape.com/\n",
        "   - Books to Scrape is another website designed for practicing web scraping. It contains a collection of books, including their titles, prices, ratings, and availability.\n"
      ],
      "metadata": {
        "id": "c854RXXl190g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Wikipedia: https://www.wikipedia.org/**"
      ],
      "metadata": {
        "id": "MM4sPTue2QwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script performs web scraping on the Wikipedia homepage (https://www.wikipedia.org/) to extract the titles of the languages available on the site.\n",
        "\n",
        "1. **Import Libraries**: The script imports the necessary libraries: `requests` for making HTTP requests and `BeautifulSoup` for parsing HTML.\n",
        "\n",
        "2. **Define URL**: The URL of the Wikipedia homepage is defined as `url`.\n",
        "\n",
        "3. **Make GET Request**: The script makes a GET request to fetch the HTML content of the Wikipedia homepage using the `requests.get()` function. The response is stored in the `response` variable.\n",
        "\n",
        "4. **Parse HTML**: The HTML content of the Wikipedia homepage is parsed using BeautifulSoup's `BeautifulSoup` function. The parsed HTML is stored in the `soup` variable.\n",
        "\n",
        "5. **Find Language Links**: BeautifulSoup's `find_all()` method is used to find all the `<a>` elements with the class `link-box`, which represent language links on the Wikipedia homepage. These links contain the titles of the languages available on the site.\n",
        "\n",
        "6. **Create CSV File**: A CSV file named \"wikipedia_languages.csv\" is created using Python's built-in `csv` module. The file is opened in write mode (`'w'`), and the column header \"Language\" is written to the file using the `writer.writerow()` method.\n",
        "\n",
        "7. **Write Language Titles to CSV**: For each language link found, the script extracts the title of the language using BeautifulSoup's various methods like `find()` and `text`. The extracted language title is then written to the CSV file using the `writer.writerow()` method.\n",
        "\n",
        "8. **Print Success Message**: Finally, a success message is printed, indicating that the language titles have been written to the CSV file.\n",
        "\n",
        "This script demonstrates a basic web scraping workflow for extracting data from a webpage and saving it to a CSV file."
      ],
      "metadata": {
        "id": "40wSAOs40JD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Define the URL of the Wikipedia homepage\n",
        "url = 'https://www.wikipedia.org/'\n",
        "\n",
        "# Make a GET request to fetch the HTML content of the page\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find the list of language links on the Wikipedia homepage\n",
        "language_links = soup.find_all('a', class_='link-box')\n",
        "\n",
        "# Create a CSV file to write the data\n",
        "filename = 'wikipedia_languages.csv'\n",
        "with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Language'])\n",
        "\n",
        "    # Write language titles to the CSV file\n",
        "    for link in language_links:\n",
        "        language = link.find('strong').text.strip()\n",
        "        writer.writerow([language])\n",
        "\n",
        "print(\"Language titles have been written to\", filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hXLjYP_l1Y9",
        "outputId": "f28baddd-0bb0-4940-d5b7-4d05c22b3015"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language titles have been written to wikipedia_languages.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quotes to Scrape: http://quotes.toscrape.com/**"
      ],
      "metadata": {
        "id": "PjwuQ5N52aIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script scrapes data from the \"Quotes to Scrape\" website (http://quotes.toscrape.com/).\n",
        "\n",
        "1. **Import Libraries**: The script imports the necessary libraries: `requests` for making HTTP requests and `BeautifulSoup` for parsing HTML.\n",
        "\n",
        "2. **Define URL**: The URL of the \"Quotes to Scrape\" website is defined as `url`.\n",
        "\n",
        "3. **Make GET Request**: The script makes a GET request to fetch the HTML content of the webpage using the `requests.get()` function. The response is stored in the `response` variable.\n",
        "\n",
        "4. **Check Response**: It checks if the request was successful by verifying the status code of the response. If the status code is 200, it prints a success message; otherwise, it prints an error message along with the status code.\n",
        "\n",
        "5. **Parse HTML**: The HTML content of the webpage is parsed using BeautifulSoup's `BeautifulSoup` function. The parsed HTML is stored in the `soup` variable.\n",
        "\n",
        "6. **Find Quote Containers**: BeautifulSoup's `find_all()` method is used to find all the `<div>` elements with the class `quote`, which represent individual quote containers on the webpage.\n",
        "\n",
        "7. **Create CSV File**: A CSV file named \"quotes_to_scrape.csv\" is created using Python's built-in `csv` module. The file is opened in write mode (`'w'`), and the column headers \"Author\", \"Quote\", and \"Tags\" are written to the file using the `writer.writerow()` method.\n",
        "\n",
        "8. **Write Quote Details to CSV**: For each quote container found, the script extracts the author name, quote text, and tags using BeautifulSoup's various methods like `find()` and `text`. The extracted details are then written to the CSV file using the `writer.writerow()` method.\n",
        "\n",
        "9. **Print Success Message**: Finally, a success message is printed, indicating that the quotes data has been written to the CSV file.\n",
        "\n",
        "This script demonstrates a basic web scraping workflow for extracting data from a webpage and saving it to a CSV file."
      ],
      "metadata": {
        "id": "wemOx5TZ0fMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Define the URL of Quotes to Scrape\n",
        "url = 'http://quotes.toscrape.com/'\n",
        "\n",
        "# Make a GET request to fetch the HTML content of the page\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    print(\"Successfully fetched the HTML content.\")\n",
        "else:\n",
        "    print(\"Failed to fetch the HTML content. Status code:\", response.status_code)\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all quote containers\n",
        "quote_containers = soup.find_all('div', class_='quote')\n",
        "\n",
        "# Create a CSV file to write the data\n",
        "filename = 'quotes_to_scrape.csv'\n",
        "with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Author', 'Quote', 'Tags'])\n",
        "\n",
        "    # Write quote details to the CSV file\n",
        "    for quote in quote_containers:\n",
        "        author = quote.find('small', class_='author').text.strip()\n",
        "        quote_text = quote.find('span', class_='text').text.strip()\n",
        "        tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
        "        writer.writerow([author, quote_text, ', '.join(tags)])\n",
        "\n",
        "print(\"Quotes data from Quotes to Scrape has been written to\", filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHKYMe-Sl1bZ",
        "outputId": "a7fc1027-6ddc-44f9-bcfe-63f86ac50656"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully fetched the HTML content.\n",
            "Quotes data from Quotes to Scrape has been written to quotes_to_scrape.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Books to Scrape: http://books.toscrape.com/**"
      ],
      "metadata": {
        "id": "vg5gEP1y2fmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script scrapes data from the \"Books to Scrape\" website (http://books.toscrape.com/).\n",
        "\n",
        "1. **Import Libraries**: The script imports the necessary libraries: `requests` for making HTTP requests and `BeautifulSoup` for parsing HTML.\n",
        "\n",
        "2. **Define URL**: The URL of the \"Books to Scrape\" website is defined as `url`.\n",
        "\n",
        "3. **Make GET Request**: The script makes a GET request to fetch the HTML content of the webpage using the `requests.get()` function. The response is stored in the `response` variable.\n",
        "\n",
        "4. **Check Response**: It checks if the request was successful by verifying the status code of the response. If the status code is 200, it prints a success message; otherwise, it prints an error message along with the status code.\n",
        "\n",
        "5. **Parse HTML**: The HTML content of the webpage is parsed using BeautifulSoup's `BeautifulSoup` function. The parsed HTML is stored in the `soup` variable.\n",
        "\n",
        "6. **Find Book Containers**: BeautifulSoup's `find_all()` method is used to find all the `<article>` elements with the class `product_pod`, which represent individual book containers on the webpage.\n",
        "\n",
        "7. **Create CSV File**: A CSV file named \"books_to_scrape.csv\" is created using Python's built-in `csv` module. The file is opened in write mode (`'w'`), and the column headers \"Title\", \"Price\", \"Rating\", and \"Availability\" are written to the file using the `writer.writerow()` method.\n",
        "\n",
        "8. **Write Book Details to CSV**: For each book container found, the script extracts the title, price, rating, and availability of the book using BeautifulSoup's various methods like `find()` and `text`. The extracted details are then written to the CSV file using the `writer.writerow()` method.\n",
        "\n",
        "9. **Print Success Message**: Finally, a success message is printed, indicating that the books data has been written to the CSV file.\n",
        "\n",
        "This script demonstrates a basic web scraping workflow for extracting data from a webpage and saving it to a CSV file."
      ],
      "metadata": {
        "id": "weacJ91p0i8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Define the URL of Books to Scrape\n",
        "url = 'http://books.toscrape.com/'\n",
        "\n",
        "# Make a GET request to fetch the HTML content of the page\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    print(\"Successfully fetched the HTML content.\")\n",
        "else:\n",
        "    print(\"Failed to fetch the HTML content. Status code:\", response.status_code)\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all book containers\n",
        "book_containers = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "# Create a CSV file to write the data\n",
        "filename = 'books_to_scrape.csv'\n",
        "with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Title', 'Price', 'Rating', 'Availability'])\n",
        "\n",
        "    # Write book details to the CSV file\n",
        "    for book in book_containers:\n",
        "        title = book.find('h3').find('a')['title']\n",
        "        price = book.find('p', class_='price_color').text\n",
        "        rating = book.find('p', class_='star-rating')['class'][1]\n",
        "        availability = book.find('p', class_='instock availability').text.strip()\n",
        "        writer.writerow([title, price, rating, availability])\n",
        "\n",
        "print(\"Books data from Books to Scrape has been written to\", filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asIe4Qipl1eJ",
        "outputId": "5601b866-c3aa-4369-e93a-2830e450f788"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully fetched the HTML content.\n",
            "Books data from Books to Scrape has been written to books_to_scrape.csv\n"
          ]
        }
      ]
    }
  ]
}